NOT QUANTISIZED

--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                aten::linear         0.06%      19.422ms        68.48%       22.467s      78.281ms      10.79 Gb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::addmm        48.87%       16.035s        68.36%       22.429s      78.151ms      10.79 Gb      10.79 Gb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::copy_        19.48%        6.392s        19.48%        6.392s      22.272ms           0 b           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      15.000us         3.39%        1.111s        1.111s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      20.000us         3.39%        1.111s        1.111s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      58.000us         3.39%        1.111s        1.111s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         3.39%        1.111s         3.39%        1.111s        1.111s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                                aten::linear         0.02%       6.079ms         3.18%        1.042s       3.631ms     143.50 Kb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::addmm         3.12%        1.024s         3.14%        1.031s       3.592ms     143.50 Kb     143.50 Kb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.01%       2.527ms         2.22%     727.998ms       2.537ms      23.30 Mb    -358.86 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.04%      14.522ms         2.21%     725.471ms       2.528ms     382.15 Mb    -463.15 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.13%      42.601ms         2.08%     681.700ms       2.375ms      20.18 Mb    -415.22 Mb           287  ...in method _native_multi_head_attention of type object at 0x7f80fc099500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      15.000us         1.75%     572.717ms     572.717ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      20.000us         1.75%     572.702ms     572.702ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      38.000us         1.75%     572.682ms     572.682ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         1.75%     572.531ms         1.75%     572.609ms     572.609ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.05%      17.908ms         1.13%     369.115ms     643.057us     382.15 Mb           0 b           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::linear         0.03%      10.826ms         1.05%     345.367ms     601.685us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                   aten::bmm         1.00%     327.273ms         1.00%     327.274ms     570.164us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::addmm         0.90%     294.393ms         0.99%     323.792ms     564.098us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 32.809s

--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                 aten::addmm        48.87%       16.035s        68.36%       22.429s      78.151ms      10.79 Gb      10.79 Gb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                                                                                                           
                                   aten::bmm         1.00%     327.273ms         1.00%     327.274ms     570.164us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                              aten::_softmax         0.75%     244.721ms         0.75%     244.721ms     852.686us     361.97 Mb     361.97 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::addmm         0.90%     294.393ms         0.99%     323.792ms     564.098us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                 aten::empty         0.01%       3.245ms         0.01%       3.245ms      11.307us     160.88 Mb     160.88 Mb           287  <built-in method contiguous of Tensor object at 0x7f8072a7fa40>              
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                         aten::empty_strided         0.01%       3.717ms         0.01%       3.717ms       6.476us     149.27 Mb     149.27 Mb           574  ...in method _native_multi_head_attention of type object at 0x7f80fc099500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                   aten::bmm         0.38%     123.153ms         0.38%     123.160ms     214.564us     120.87 Mb     120.87 Mb           574  ...in method _native_multi_head_attention of type object at 0x7f80fc099500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                         aten::empty_strided         0.01%       3.368ms         0.01%       3.368ms      11.735us     120.03 Mb     120.03 Mb           287  <built-in method clone of Tensor object at 0x7f80733e4950>                   
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                              demo.py(268): main                                                           
                                                                                                                                                                                                                                           
                                   aten::div         0.10%      33.602ms         0.13%      43.940ms      76.285us     101.18 Mb     101.18 Mb           576  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::empty         0.01%       1.800ms         0.01%       1.800ms       6.272us      90.01 Mb      90.01 Mb           287  <built-in method ones of type object at 0x7f80fc099500>                      
                                                                                                                                                              faceformer.py(34): enc_dec_mask                                              
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                           
                         aten::empty_strided         0.00%       1.422ms         0.00%       1.422ms       4.955us      88.31 Mb      88.31 Mb           287  <built-in method zeros_like of type object at 0x7f80fc099500>                
                                                                                                                                                              torch/nn/functional.py(4982): _canonical_mask                                
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::empty         0.02%       5.372ms         0.02%       5.372ms       9.375us      80.72 Mb      80.72 Mb           573  ...in method _native_multi_head_attention of type object at 0x7f80fc099500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                   aten::add         0.55%     180.549ms         0.55%     180.549ms     626.906us      76.90 Mb      76.90 Mb           288  faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                              demo.py(268): main                                                           
                                                                                                                                                              demo.py(272): <module>                                                       
                                                                                                                                                                                                                                           
                                 aten::empty         0.00%      44.000us         0.00%      44.000us      14.667us      71.98 Mb      71.98 Mb             3  <built-in method group_norm of type object at 0x7f80fc099500>                
                                                                                                                                                              torch/nn/functional.py(2518): group_norm                                     
                                                                                                                                                              torch/nn/modules/normalization.py(272): forward                              
                                                                                                                                                              nn.Module: GroupNorm_0                                                       
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(183): forward              
                                                                                                                                                                                                                                           
                                 aten::empty         0.00%      57.000us         0.00%      57.000us      28.500us      71.98 Mb      71.98 Mb             2  <built-in method conv1d of type object at 0x7f80fc099500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_0                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(183): forward              
                                                                                                                                                                                                                                           
                                  aten::gelu         0.35%     116.221ms         0.35%     116.221ms     116.221ms      71.98 Mb      71.98 Mb             1  <built-in function gelu>                                                     
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(183): forward              
                                                                                                                                                              nn.Module: Wav2Vec2GroupNormConvLayer_0                                      
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(250): forward              
                                                                                                                                                              nn.Module: Wav2Vec2FeatureExtractor_0                                        
                                                                                                                                                                                                                                           
                                    aten::mm         0.33%     107.646ms         0.33%     107.713ms     375.307us      60.54 Mb      60.54 Mb           287  ...in method _native_multi_head_attention of type object at 0x7f80fc099500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                   aten::add         0.08%      24.807ms         0.08%      24.807ms      28.812us      60.54 Mb      60.54 Mb           861  torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                              nn.Module: TransformerDecoderLayer_0                                         
                                                                                                                                                              torch/nn/modules/transformer.py(350): forward                                
                                                                                                                                                              nn.Module: TransformerDecoder_0                                              
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                                                                                                           
                                 aten::addmm         0.21%      68.034ms         0.23%      75.784ms     264.056us      40.36 Mb      40.36 Mb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_75                                                         
                                                                                                                                                              torch/nn/modules/transformer.py(743): _ff_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                             aten::clamp_min         0.03%       9.183ms         0.03%       9.183ms      31.997us      40.36 Mb      40.36 Mb           287  <built-in method relu of type object at 0x7f80fc099500>                      
                                                                                                                                                              torch/nn/functional.py(1446): relu                                           
                                                                                                                                                              torch/nn/modules/transformer.py(743): _ff_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                              nn.Module: TransformerDecoderLayer_0                                         
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 32.809s

Time for prediction: 139.24962258338928
rendering:  test





QUANTISIZED

--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                   quantized::linear_dynamic        74.53%       23.928s        74.57%       23.941s      83.417ms      10.79 Gb     -10.79 Gb           287  <built-in method linear_dynamic of PyCapsule object at 0x7f807467a270>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      17.000us         3.36%        1.079s        1.079s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      19.000us         3.36%        1.079s        1.079s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      53.000us         3.36%        1.079s        1.079s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         3.36%        1.078s         3.36%        1.078s        1.078s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.01%       2.397ms         2.29%     734.497ms       2.559ms      28.04 Mb    -354.18 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.05%      15.823ms         2.28%     732.100ms       2.551ms     382.22 Mb    -462.53 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.13%      40.931ms         2.12%     681.868ms       2.376ms      20.18 Mb    -415.08 Mb           287  ...in method _native_multi_head_attention of type object at 0x7f80d834d500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      13.000us         1.56%     502.324ms     502.324ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      17.000us         1.56%     502.311ms     502.311ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      43.000us         1.56%     502.294ms     502.294ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         1.56%     502.161ms         1.56%     502.217ms     502.217ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(133): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.06%      18.015ms         1.15%     370.254ms     645.042us     382.15 Mb           0 b           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::linear         0.03%      10.619ms         1.05%     338.592ms     589.882us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                   aten::bmm         1.02%     328.528ms         1.02%     328.528ms     572.348us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                   quantized::linear_dynamic         0.99%     316.934ms         1.01%     324.961ms       1.132ms     143.50 Kb    -143.50 Kb           287  <built-in method linear_dynamic of PyCapsule object at 0x7f807467a270>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                                                                                                           
                                 aten::addmm         0.90%     288.610ms         0.99%     316.404ms     551.226us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      12.000us         0.87%     278.311ms     278.311ms       1.68 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(204): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      20.000us         0.87%     278.299ms     278.299ms       1.68 Mb           0 b             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(204): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      69.000us         0.87%     278.279ms     278.279ms       1.68 Mb      -1.68 Mb             1  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(204): forward              
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 32.104s

--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                 aten::empty         0.03%      10.186ms         0.03%      10.186ms      17.746us      21.33 Gb      21.33 Gb           574  <built-in method linear_dynamic of PyCapsule object at 0x7f807467a270>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                                                                                                           
                                   aten::bmm         1.02%     328.528ms         1.02%     328.528ms     572.348us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                              aten::_softmax         0.77%     248.363ms         0.77%     248.363ms     865.376us     361.97 Mb     361.97 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                            aten::empty_like         0.01%       2.242ms         0.02%       5.611ms      19.551us      10.79 Gb     262.63 Mb           287  <built-in method linear_dynamic of PyCapsule object at 0x7f807467a270>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                                                                                                           
                                 aten::addmm         0.90%     288.610ms         0.99%     316.404ms     551.226us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                 aten::empty         0.01%       3.602ms         0.01%       3.602ms      12.551us     160.88 Mb     160.88 Mb           287  <built-in method contiguous of Tensor object at 0x7f8076ac1270>              
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                         aten::empty_strided         0.01%       3.934ms         0.01%       3.934ms       6.854us     148.41 Mb     148.41 Mb           574  ...in method _native_multi_head_attention of type object at 0x7f80d834d500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                         aten::empty_strided         0.01%       3.363ms         0.01%       3.363ms      11.718us     120.87 Mb     120.87 Mb           287  <built-in method clone of Tensor object at 0x7f804d4307c0>                   
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                              demo.py(268): main                                                           
                                                                                                                                                                                                                                           
                                   aten::bmm         0.38%     121.903ms         0.38%     121.914ms     212.394us     120.87 Mb     120.87 Mb           574  ...in method _native_multi_head_attention of type object at 0x7f80d834d500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                   aten::div         0.10%      33.276ms         0.14%      44.171ms      76.953us     100.62 Mb     100.62 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::empty         0.01%       1.837ms         0.01%       1.837ms       6.401us      90.49 Mb      90.49 Mb           287  <built-in method ones of type object at 0x7f80d834d500>                      
                                                                                                                                                              faceformer.py(34): enc_dec_mask                                              
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                           
                         aten::empty_strided         0.00%       1.506ms         0.00%       1.506ms       5.247us      90.09 Mb      90.09 Mb           287  <built-in method zeros_like of type object at 0x7f80d834d500>                
                                                                                                                                                              torch/nn/functional.py(4982): _canonical_mask                                
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::empty         0.02%       5.132ms         0.02%       5.132ms       8.956us      80.72 Mb      80.72 Mb           573  ...in method _native_multi_head_attention of type object at 0x7f80d834d500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                 aten::empty         0.01%       2.449ms         0.01%       2.449ms       4.267us      78.06 Mb      78.06 Mb           574  <built-in method linear_dynamic of PyCapsule object at 0x7f807467a270>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_75                                                         
                                                                                                                                                              torch/nn/modules/transformer.py(743): _ff_block                              
                                                                                                                                                                                                                                           
                                   aten::add         0.23%      75.048ms         0.23%      75.048ms     260.583us      76.90 Mb      76.90 Mb           288  faceformer.py(137): predict                                                  
                                                                                                                                                              demo.py(86): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                              demo.py(268): main                                                           
                                                                                                                                                              demo.py(272): <module>                                                       
                                                                                                                                                                                                                                           
                                 aten::empty         0.00%      32.000us         0.00%      32.000us      10.667us      71.98 Mb      71.98 Mb             3  <built-in method group_norm of type object at 0x7f80d834d500>                
                                                                                                                                                              torch/nn/functional.py(2518): group_norm                                     
                                                                                                                                                              torch/nn/modules/normalization.py(272): forward                              
                                                                                                                                                              nn.Module: GroupNorm_0                                                       
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(183): forward              
                                                                                                                                                                                                                                           
                                 aten::empty         0.00%      45.000us         0.00%      45.000us      22.500us      71.98 Mb      71.98 Mb             2  <built-in method conv1d of type object at 0x7f80d834d500>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_0                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(183): forward              
                                                                                                                                                                                                                                           
                                  aten::gelu         0.18%      57.714ms         0.18%      57.714ms      57.714ms      71.98 Mb      71.98 Mb             1  <built-in function gelu>                                                     
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(183): forward              
                                                                                                                                                              nn.Module: Wav2Vec2GroupNormConvLayer_0                                      
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(250): forward              
                                                                                                                                                              nn.Module: Wav2Vec2FeatureExtractor_0                                        
                                                                                                                                                                                                                                           
                                    aten::mm         0.34%     108.462ms         0.34%     108.562ms     378.265us      60.54 Mb      60.54 Mb           287  ...in method _native_multi_head_attention of type object at 0x7f80d834d500>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                   aten::add         0.07%      23.825ms         0.07%      23.825ms      27.671us      60.54 Mb      60.54 Mb           861  torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                              nn.Module: TransformerDecoderLayer_0                                         
                                                                                                                                                              torch/nn/modules/transformer.py(350): forward                                
                                                                                                                                                              nn.Module: TransformerDecoder_0                                              
                                                                                                                                                              faceformer.py(137): predict                                                  
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 32.104s
